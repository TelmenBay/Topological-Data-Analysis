{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance\n",
    "%pip install matplotlib\n",
    "%pip install pandas numpy\n",
    "%pip install --upgrade sympy torch\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# 3. Restart runtime again\n",
    "\n",
    "# 4. Now import and run your code\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE NBEATS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==================== IMPROVED NBEATS COMPONENTS ====================\n",
    "\n",
    "class NBeatsBlock(nn.Module):\n",
    "    \"\"\"Single NBEATS block with proper basis functions\"\"\"\n",
    "    def __init__(self, input_size, theta_size, horizon, n_layers=4, layer_size=256,\n",
    "                 basis_function=None, share_weights=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.horizon = horizon\n",
    "        self.basis_function = basis_function\n",
    "\n",
    "        # Fully connected stack with layer normalization\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(layer_size, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        self.fc_stack = nn.Sequential(*layers)\n",
    "\n",
    "        # Theta layers for backcast and forecast\n",
    "        self.theta_b = nn.Linear(layer_size, theta_size, bias=False)\n",
    "        self.theta_f = nn.Linear(layer_size, theta_size, bias=False)\n",
    "\n",
    "        # If no basis function provided, use generic linear basis\n",
    "        if basis_function is None:\n",
    "            self.backcast_basis = nn.Linear(theta_size, input_size)\n",
    "            self.forecast_basis = nn.Linear(theta_size, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_size]\n",
    "        h = self.fc_stack(x)\n",
    "\n",
    "        theta_b = self.theta_b(h)  # [batch_size, theta_size]\n",
    "        theta_f = self.theta_f(h)  # [batch_size, theta_size]\n",
    "\n",
    "        if self.basis_function is not None:\n",
    "            # Interpretable mode (trend/seasonality)\n",
    "            backcast = self.basis_function(theta_b, self.input_size)\n",
    "            forecast = self.basis_function(theta_f, self.horizon)\n",
    "        else:\n",
    "            # Generic mode with proper linear projection\n",
    "            backcast = self.backcast_basis(theta_b)\n",
    "            forecast = self.forecast_basis(theta_f)\n",
    "\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class TrendBasis(nn.Module):\n",
    "    \"\"\"Polynomial trend basis function\"\"\"\n",
    "    def __init__(self, degree=3):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "\n",
    "    def forward(self, theta, length):\n",
    "        # theta: [batch_size, degree+1]\n",
    "        batch_size = theta.size(0)\n",
    "        T = torch.arange(length, dtype=torch.float32, device=theta.device) / length\n",
    "        T = T.unsqueeze(0).repeat(batch_size, 1)  # [batch_size, length]\n",
    "\n",
    "        # Create polynomial basis [1, t, t^2, t^3, ...]\n",
    "        basis = torch.stack([T**i for i in range(self.degree + 1)], dim=2)  # [batch_size, length, degree+1]\n",
    "        output = torch.einsum('btd,bd->bt', basis, theta)  # [batch_size, length]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SeasonalityBasis(nn.Module):\n",
    "    \"\"\"Fourier series basis function for seasonality\"\"\"\n",
    "    def __init__(self, harmonics=10):\n",
    "        super().__init__()\n",
    "        self.harmonics = harmonics\n",
    "\n",
    "    def forward(self, theta, length):\n",
    "        # theta: [batch_size, 2*harmonics]\n",
    "        batch_size = theta.size(0)\n",
    "        T = torch.arange(length, dtype=torch.float32, device=theta.device) / length\n",
    "        T = T.unsqueeze(0).repeat(batch_size, 1)  # [batch_size, length]\n",
    "\n",
    "        # Create Fourier basis [cos(2Ï€t), sin(2Ï€t), cos(4Ï€t), sin(4Ï€t), ...]\n",
    "        basis = []\n",
    "        for i in range(1, self.harmonics + 1):\n",
    "            basis.append(torch.cos(2 * np.pi * i * T))\n",
    "            basis.append(torch.sin(2 * np.pi * i * T))\n",
    "\n",
    "        basis = torch.stack(basis, dim=2)  # [batch_size, length, 2*harmonics]\n",
    "        output = torch.einsum('btd,bd->bt', basis, theta)  # [batch_size, length]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class NBeatsStack(nn.Module):\n",
    "    \"\"\"Stack of NBEATS blocks with proper residual connections\"\"\"\n",
    "    def __init__(self, input_size, horizon, n_blocks=3, n_layers=4, layer_size=256,\n",
    "                 stack_type='generic', share_weights=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.stack_type = stack_type\n",
    "\n",
    "        # Define basis function and theta size based on stack type\n",
    "        if stack_type == 'trend':\n",
    "            degree = 3\n",
    "            basis_function = TrendBasis(degree=degree)\n",
    "            theta_size = degree + 1\n",
    "        elif stack_type == 'seasonality':\n",
    "            harmonics = int(input_size / 2)  # Adaptive harmonics\n",
    "            basis_function = SeasonalityBasis(harmonics=harmonics)\n",
    "            theta_size = 2 * harmonics\n",
    "        else:  # generic\n",
    "            basis_function = None\n",
    "            theta_size = max(input_size // 4, horizon // 4)  # Smaller theta size\n",
    "\n",
    "        # Create blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(input_size, theta_size, horizon, n_layers, layer_size,\n",
    "                       basis_function, share_weights)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_size]\n",
    "        residual = x\n",
    "        forecast = torch.zeros(x.size(0), self.horizon, device=x.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            backcast, block_forecast = block(residual)\n",
    "            residual = residual - backcast  # Remove explained signal\n",
    "            forecast = forecast + block_forecast  # Accumulate forecast\n",
    "\n",
    "        return forecast, residual  # Return both forecast and final residual\n",
    "\n",
    "\n",
    "class NBEATS(nn.Module):\n",
    "    \"\"\"\n",
    "    Full NBEATS model with multiple stacks and doubly residual stacking\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, horizon, stack_types=['trend', 'seasonality'],\n",
    "                 n_blocks=[3, 3], n_layers=4, layer_size=256, share_weights=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "\n",
    "        assert len(stack_types) == len(n_blocks), \"stack_types and n_blocks must have same length\"\n",
    "\n",
    "        self.stacks = nn.ModuleList([\n",
    "            NBeatsStack(input_size, horizon, n_blocks[i], n_layers, layer_size,\n",
    "                       stack_types[i], share_weights)\n",
    "            for i in range(len(stack_types))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_size]\n",
    "        residual = x\n",
    "        forecast = torch.zeros(x.size(0), self.horizon, device=x.device)\n",
    "\n",
    "        # Doubly residual stacking\n",
    "        for stack in self.stacks:\n",
    "            stack_forecast, residual = stack(residual)\n",
    "            forecast = forecast + stack_forecast\n",
    "\n",
    "        return forecast\n",
    "\n",
    "\n",
    "# ==================== IMPROVED DATASET ====================\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom dataset for time series forecasting with proper indexing\"\"\"\n",
    "    def __init__(self, data, input_size, horizon):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_size - self.horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.input_size]\n",
    "        y = self.data[idx + self.input_size:idx + self.input_size + self.horizon]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ==================== TRAINING WITH EARLY STOPPING ====================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Download and prepare data\n",
    "\n",
    "    ticker = 'GOOG'\n",
    "    print(f\"\\nDownloading {ticker} stock data...\")\n",
    "    data = yf.download(ticker, start='2015-01-01', end='2023-12-31', auto_adjust=True)\n",
    "\n",
    "    # Flatten MultiIndex if present\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "    # Use Close price\n",
    "    prices = data['Close'].values\n",
    "\n",
    "    # Normalize using z-score normalization\n",
    "    mean, std = prices.mean(), prices.std()\n",
    "    prices_normalized = (prices - mean) / std\n",
    "\n",
    "    print(f\"Data shape: {prices.shape}\")\n",
    "    print(f\"Price range: ${prices.min():.2f} - ${prices.max():.2f}\")\n",
    "    print(f\"Mean: ${mean:.2f}, Std: ${std:.2f}\")\n",
    "\n",
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    train_size = int(0.7 * len(prices_normalized))\n",
    "    val_size = int(0.15 * len(prices_normalized))\n",
    "\n",
    "    train_data = prices_normalized[:train_size]\n",
    "    val_data = prices_normalized[train_size:train_size + val_size]\n",
    "    test_data = prices_normalized[train_size + val_size:]\n",
    "\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "    # Hyperparameters - ADJUSTED TO PREVENT OVERFITTING\n",
    "    INPUT_SIZE = 60      # lookback window\n",
    "    HORIZON = 20         # forecast horizon\n",
    "    BATCH_SIZE = 32      # Smaller batch size\n",
    "    EPOCHS = 100         # More epochs with early stopping\n",
    "    LEARNING_RATE = 1e-3\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(train_data, INPUT_SIZE, HORIZON)\n",
    "    val_dataset = TimeSeriesDataset(val_data, INPUT_SIZE, HORIZON)\n",
    "    test_dataset = TimeSeriesDataset(test_data, INPUT_SIZE, HORIZON)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    # Create improved model with better architecture\n",
    "    model = NBEATS(\n",
    "        input_size=INPUT_SIZE,\n",
    "        horizon=HORIZON,\n",
    "        stack_types=['trend', 'seasonality'],\n",
    "        n_blocks=[2, 2],      # Reduced blocks\n",
    "        n_layers=4,\n",
    "        layer_size=128        # Reduced layer size\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nðŸ¤– Model created with {total_params:,} parameters\")\n",
    "\n",
    "    # Loss and optimizer with weight decay (L2 regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                      factor=0.5, patience=5)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=15, min_delta=1e-5)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), '/tmp/best_nbeats_model.pt')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train: {train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\nâš ï¸  Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('/tmp/best_nbeats_model.pt'))\n",
    "    print(\"\\nTraining complete! Loaded best model.\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "    plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('NBEATS Training Progress (Improved)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')  # Log scale to see details\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions on test set\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            pred = model(x).cpu().numpy()\n",
    "            all_predictions.append(pred)\n",
    "            all_targets.append(y.numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Denormalize predictions\n",
    "    predictions_denorm = predictions * std + mean\n",
    "    targets_denorm = targets * std + mean\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = np.mean(np.abs(targets_denorm - predictions_denorm))\n",
    "    rmse = np.sqrt(np.mean((targets_denorm - predictions_denorm) ** 2))\n",
    "    mape = np.mean(np.abs((targets_denorm - predictions_denorm) / targets_denorm)) * 100\n",
    "\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  MAE:  ${mae:.2f}\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "    # Plot sample predictions\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Show first 5 predictions\n",
    "    for i in range(min(5, len(predictions_denorm))):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.plot(targets_denorm[i], 'b-', label='Actual', linewidth=2)\n",
    "        plt.plot(predictions_denorm[i], 'r--', label='Predicted', linewidth=2)\n",
    "        plt.title(f'Sample {i+1}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Price ($)')\n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Make future forecast\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        last_window = torch.FloatTensor(test_data[-INPUT_SIZE:]).unsqueeze(0).to(device)\n",
    "        future_prediction = model(last_window).cpu().numpy()[0]\n",
    "    future_prediction_denorm = future_prediction * std + mean\n",
    "\n",
    "    print(f\"\\nFuture forecast for next {HORIZON} days:\")\n",
    "    print(future_prediction_denorm)\n",
    "\n",
    "    # Plot future forecast\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    historical = test_data[-100:] * std + mean\n",
    "    plt.plot(range(len(historical)), historical, 'b-', label='Historical', linewidth=2)\n",
    "    plt.plot(range(len(historical), len(historical) + HORIZON),\n",
    "             future_prediction_denorm, 'r--', label='Forecast', linewidth=2, marker='o')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.title(f'{ticker} Stock Price Forecast (Improved Model)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    model, train_losses, test_losses = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
