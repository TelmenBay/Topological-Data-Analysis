{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install yfinance\n",
    "%pip install matplotlib\n",
    "%pip install pandas numpy\n",
    "%pip install --upgrade sympy torch\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# 3. Restart runtime again\n",
    "\n",
    "# 4. Now import and run your code\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-BEATS + Topological Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NBEATS + TDA Standalone Implementation\n",
    "Train only the TDA-enhanced model and save results for comparison\n",
    "Uses ripser for real persistent homology computation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Install: !pip install ripser\n",
    "\n",
    "try:\n",
    "    from ripser import ripser\n",
    "    RIPSER_AVAILABLE = True\n",
    "    print(\"Ripser library available\")\n",
    "except ImportError:\n",
    "    RIPSER_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Install ripser: pip install ripser\")\n",
    "    exit()\n",
    "\n",
    "# ==================== TDA FROM SCRATCH ====================\n",
    "\n",
    "def takens_embedding(time_series, embedding_dim=3, time_delay=1):\n",
    "    \"\"\"Create Takens embedding (phase space reconstruction)\"\"\"\n",
    "    n = len(time_series)\n",
    "    m = n - (embedding_dim - 1) * time_delay\n",
    "    \n",
    "    if m <= 0:\n",
    "        raise ValueError(\"Time series too short for given embedding parameters\")\n",
    "    \n",
    "    embedded = np.zeros((m, embedding_dim))\n",
    "    for i in range(m):\n",
    "        for j in range(embedding_dim):\n",
    "            embedded[i, j] = time_series[i + j * time_delay]\n",
    "    \n",
    "    return embedded\n",
    "\n",
    "\n",
    "def compute_persistence_diagrams(point_cloud, max_dimension=1):\n",
    "    \"\"\"Compute persistence diagrams using Vietoris-Rips complex\"\"\"\n",
    "    if not RIPSER_AVAILABLE:\n",
    "        raise ImportError(\"ripser not installed\")\n",
    "    \n",
    "    from ripser import ripser as ripser_compute\n",
    "    result = ripser_compute(point_cloud, maxdim=max_dimension, thresh=np.inf)\n",
    "    return result['dgms']\n",
    "\n",
    "\n",
    "def persistence_entropy(diagram):\n",
    "    \"\"\"Compute persistence entropy\"\"\"\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    diagram = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    lifetimes = lifetimes[lifetimes > 0]\n",
    "    \n",
    "    if len(lifetimes) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lifetimes = lifetimes / np.sum(lifetimes)\n",
    "    entropy = -np.sum(lifetimes * np.log(lifetimes + 1e-10))\n",
    "    \n",
    "    return float(entropy)\n",
    "\n",
    "\n",
    "def max_persistence(diagram):\n",
    "    \"\"\"Maximum persistence (lifetime) of any feature\"\"\"\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    diagram = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    return float(np.max(lifetimes))\n",
    "\n",
    "\n",
    "def total_persistence(diagram):\n",
    "    \"\"\"Total persistence (sum of all lifetimes)\"\"\"\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    diagram = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "    if len(diagram) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    return float(np.sum(lifetimes))\n",
    "\n",
    "\n",
    "def number_of_features(diagram, threshold=0.0):\n",
    "    \"\"\"Count features with persistence above threshold\"\"\"\n",
    "    if len(diagram) == 0:\n",
    "        return 0\n",
    "    \n",
    "    diagram = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "    if len(diagram) == 0:\n",
    "        return 0\n",
    "    \n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    return int(np.sum(lifetimes > threshold))\n",
    "\n",
    "\n",
    "def betti_numbers(diagram, t):\n",
    "    \"\"\"Compute Betti number at filtration value t\"\"\"\n",
    "    if len(diagram) == 0:\n",
    "        return 0\n",
    "    \n",
    "    diagram = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "    alive = np.sum((diagram[:, 0] <= t) & (diagram[:, 1] > t))\n",
    "    \n",
    "    return int(alive)\n",
    "\n",
    "\n",
    "class TDAFeatureExtractor:\n",
    "    \"\"\"Complete TDA feature extractor using ripser\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=3, time_delay=1, max_homology_dim=1):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.time_delay = time_delay\n",
    "        self.max_homology_dim = max_homology_dim\n",
    "        \n",
    "    def extract_features(self, time_series):\n",
    "        \"\"\"\n",
    "        Extract 12 TDA features from time series:\n",
    "        H0: entropy, max_pers, total_pers, n_features, betti, amplitude\n",
    "        H1: entropy, max_pers, total_pers, n_features, betti, amplitude\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Takens embedding\n",
    "            embedded = takens_embedding(time_series, self.embedding_dim, self.time_delay)\n",
    "            \n",
    "            # Compute persistence diagrams\n",
    "            diagrams = compute_persistence_diagrams(embedded, max_dimension=self.max_homology_dim)\n",
    "            \n",
    "            # Extract features\n",
    "            features = []\n",
    "            \n",
    "            for dim in range(self.max_homology_dim + 1):\n",
    "                diagram = diagrams[dim]\n",
    "                \n",
    "                # 1. Entropy\n",
    "                features.append(persistence_entropy(diagram))\n",
    "                \n",
    "                # 2. Max persistence\n",
    "                max_pers = max_persistence(diagram)\n",
    "                features.append(max_pers)\n",
    "                \n",
    "                # 3. Total persistence\n",
    "                features.append(total_persistence(diagram))\n",
    "                \n",
    "                # 4. Number of significant features (threshold = 10% of max)\n",
    "                threshold = 0.1 * max_pers if max_pers > 0 else 0.0\n",
    "                features.append(float(number_of_features(diagram, threshold)))\n",
    "                \n",
    "                # 5. Betti number at midpoint\n",
    "                if len(diagram) > 0:\n",
    "                    diagram_finite = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "                    if len(diagram_finite) > 0:\n",
    "                        midpoint = (np.min(diagram_finite[:, 0]) + np.max(diagram_finite[:, 1])) / 2\n",
    "                        features.append(float(betti_numbers(diagram, midpoint)))\n",
    "                    else:\n",
    "                        features.append(0.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "                \n",
    "                # 6. Amplitude (max death - min birth)\n",
    "                if len(diagram) > 0:\n",
    "                    diagram_finite = diagram[np.isfinite(diagram).all(axis=1)]\n",
    "                    if len(diagram_finite) > 0:\n",
    "                        amplitude = np.max(diagram_finite[:, 1]) - np.min(diagram_finite[:, 0])\n",
    "                        features.append(float(amplitude))\n",
    "                    else:\n",
    "                        features.append(0.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "            \n",
    "            return np.array(features[:12], dtype=np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: TDA extraction failed: {e}\")\n",
    "            return np.zeros(12, dtype=np.float32)\n",
    "\n",
    "\n",
    "# ==================== NBEATS COMPONENTS ====================\n",
    "\n",
    "class NBeatsBlock(nn.Module):\n",
    "    \"\"\"NBEATS block with proper basis functions\"\"\"\n",
    "    def __init__(self, input_size, theta_size, horizon, n_layers=4, layer_size=256):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        layers = [nn.Linear(input_size, layer_size), nn.ReLU()]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.extend([nn.Linear(layer_size, layer_size), nn.ReLU()])\n",
    "        \n",
    "        self.fc_stack = nn.Sequential(*layers)\n",
    "        self.theta_b = nn.Linear(layer_size, theta_size, bias=False)\n",
    "        self.theta_f = nn.Linear(layer_size, theta_size, bias=False)\n",
    "        \n",
    "        self.backcast_basis = nn.Linear(theta_size, input_size)\n",
    "        self.forecast_basis = nn.Linear(theta_size, horizon)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.fc_stack(x)\n",
    "        theta_b = self.theta_b(h)\n",
    "        theta_f = self.theta_f(h)\n",
    "        \n",
    "        backcast = self.backcast_basis(theta_b)\n",
    "        forecast = self.forecast_basis(theta_f)\n",
    "        \n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class NBeatsStack(nn.Module):\n",
    "    \"\"\"Stack of NBEATS blocks\"\"\"\n",
    "    def __init__(self, input_size, horizon, n_blocks=3, n_layers=4, layer_size=256):\n",
    "        super().__init__()\n",
    "        theta_size = max(input_size // 4, horizon // 4)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(input_size, theta_size, horizon, n_layers, layer_size)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        forecast = torch.zeros(x.size(0), self.blocks[0].horizon, device=x.device)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            backcast, block_forecast = block(residual)\n",
    "            residual = residual - backcast\n",
    "            forecast = forecast + block_forecast\n",
    "            \n",
    "        return forecast, residual\n",
    "\n",
    "\n",
    "class NBEATS_TDA(nn.Module):\n",
    "    \"\"\"NBEATS enhanced with TDA features\"\"\"\n",
    "    def __init__(self, input_size, horizon, tda_feature_size=12, \n",
    "                 n_stacks=2, n_blocks=2, n_layers=4, layer_size=128):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        # NBEATS backbone\n",
    "        self.stacks = nn.ModuleList([\n",
    "            NBeatsStack(input_size, horizon, n_blocks, n_layers, layer_size)\n",
    "            for _ in range(n_stacks)\n",
    "        ])\n",
    "        \n",
    "        # TDA feature processor\n",
    "        self.tda_processor = nn.Sequential(\n",
    "            nn.Linear(tda_feature_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, horizon)\n",
    "        )\n",
    "        \n",
    "        # Attention fusion\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(horizon * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.output_projection = nn.Linear(horizon, horizon)\n",
    "        \n",
    "    def forward(self, x, tda_features):\n",
    "        # NBEATS forecast\n",
    "        residual = x\n",
    "        nbeats_forecast = torch.zeros(x.size(0), self.horizon, device=x.device)\n",
    "        \n",
    "        for stack in self.stacks:\n",
    "            stack_forecast, residual = stack(residual)\n",
    "            nbeats_forecast = nbeats_forecast + stack_forecast\n",
    "        \n",
    "        # TDA forecast\n",
    "        tda_forecast = self.tda_processor(tda_features)\n",
    "        \n",
    "        # Attention fusion\n",
    "        combined = torch.cat([nbeats_forecast, tda_forecast], dim=1)\n",
    "        attention_weights = self.attention(combined)\n",
    "        \n",
    "        weighted_nbeats = nbeats_forecast * attention_weights[:, 0:1]\n",
    "        weighted_tda = tda_forecast * attention_weights[:, 1:2]\n",
    "        \n",
    "        fused = weighted_nbeats + weighted_tda\n",
    "        final_forecast = self.output_projection(fused)\n",
    "        \n",
    "        return final_forecast\n",
    "\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "\n",
    "class TDADataset(Dataset):\n",
    "    \"\"\"Dataset with TDA features\"\"\"\n",
    "    def __init__(self, data, input_size, horizon, tda_extractor):\n",
    "        self.data = data\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.tda_extractor = tda_extractor\n",
    "        \n",
    "        print(\"Pre-computing TDA features...\")\n",
    "        self.tda_features = self._precompute_tda()\n",
    "        print(f\"TDA features computed: shape {self.tda_features.shape}\")\n",
    "        \n",
    "    def _precompute_tda(self):\n",
    "        features_list = []\n",
    "        total = len(self)\n",
    "        \n",
    "        for i in range(total):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"  {i+1}/{total}...\")\n",
    "            \n",
    "            window = self.data[i:i + self.input_size]\n",
    "            features = self.tda_extractor.extract_features(window)\n",
    "            features_list.append(features)\n",
    "        \n",
    "        return torch.FloatTensor(np.array(features_list))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_size - self.horizon + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.data[idx:idx + self.input_size])\n",
    "        y = torch.FloatTensor(self.data[idx + self.input_size:idx + self.input_size + self.horizon])\n",
    "        tda_feat = self.tda_features[idx]\n",
    "        return x, y, tda_feat\n",
    "\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-5):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    \"\"\"Train NBEATS+TDA model\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    early_stopping = EarlyStopping(patience=15)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y, tda_feat in train_loader:\n",
    "            x, y, tda_feat = x.to(device), y.to(device), tda_feat.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, tda_feat)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y, tda_feat in val_loader:\n",
    "                x, y, tda_feat = x.to(device), y.to(device), tda_feat.to(device)\n",
    "                output = model(x, tda_feat)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), '/tmp/best_nbeats_tda.pt')\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Train: {train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "        \n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"‚ö†Ô∏è  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('/tmp/best_nbeats_tda.pt'))\n",
    "    print(\"Training complete! Loaded best model.\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate and get predictions\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, tda_feat in test_loader:\n",
    "            x, tda_feat = x.to(device), tda_feat.to(device)\n",
    "            output = model(x, tda_feat)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute MAE, RMSE, MAPE\"\"\"\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\\n\")\n",
    "    \n",
    "    # Download data\n",
    "    ticker = 'GOOG'\n",
    "    print(f\"üìä Downloading {ticker} stock data...\")\n",
    "    data = yf.download(ticker, start='2015-01-01', end='2023-12-31', auto_adjust=True)\n",
    "    \n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        data.columns = data.columns.get_level_values(0)\n",
    "    \n",
    "    prices = data['Close'].values\n",
    "    mean, std = prices.mean(), prices.std()\n",
    "    prices_normalized = (prices - mean) / std\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.7 * len(prices_normalized))\n",
    "    val_size = int(0.15 * len(prices_normalized))\n",
    "    \n",
    "    train_data = prices_normalized[:train_size]\n",
    "    val_data = prices_normalized[train_size:train_size + val_size]\n",
    "    test_data = prices_normalized[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\\n\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    INPUT_SIZE = 60\n",
    "    HORIZON = 20\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 100\n",
    "    LR = 1e-3\n",
    "    \n",
    "    # Create TDA extractor (using ripser-based implementation)\n",
    "    print(\"Initializing TDA Feature Extractor with Ripser...\")\n",
    "    tda_extractor = TDAFeatureExtractor(embedding_dim=3, time_delay=1, max_homology_dim=1)\n",
    "    print(\"Using real persistent homology computation\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TDADataset(train_data, INPUT_SIZE, HORIZON, tda_extractor)\n",
    "    val_dataset = TDADataset(val_data, INPUT_SIZE, HORIZON, tda_extractor)\n",
    "    test_dataset = TDADataset(test_data, INPUT_SIZE, HORIZON, tda_extractor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"\\nTraining NBEATS+TDA Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create model\n",
    "    model = NBEATS_TDA(INPUT_SIZE, HORIZON, tda_feature_size=12, \n",
    "                       n_stacks=2, n_blocks=2, layer_size=128)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n",
    "    \n",
    "    # Train\n",
    "    model, train_losses, val_losses = train_model(model, train_loader, val_loader, \n",
    "                                                    EPOCHS, LR, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    predictions, targets = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Denormalize\n",
    "    predictions_denorm = predictions * std + mean\n",
    "    targets_denorm = targets * std + mean\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(targets_denorm, predictions_denorm)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NBEATS+TDA TEST RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  MAE:  ${metrics['MAE']:.2f}\")\n",
    "    print(f\"  RMSE: ${metrics['RMSE']:.2f}\")\n",
    "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save results for comparison\n",
    "    results = {\n",
    "        'model': 'NBEATS+TDA',\n",
    "        'ticker': ticker,\n",
    "        'input_size': INPUT_SIZE,\n",
    "        'horizon': HORIZON,\n",
    "        'test_samples': len(test_dataset),\n",
    "        'metrics': {\n",
    "            'MAE': float(metrics['MAE']),\n",
    "            'RMSE': float(metrics['RMSE']),\n",
    "            'MAPE': float(metrics['MAPE'])\n",
    "        },\n",
    "        'hyperparameters': {\n",
    "            'n_stacks': 2,\n",
    "            'n_blocks': 2,\n",
    "            'layer_size': 128,\n",
    "            'tda_features': 12,\n",
    "            'embedding_dim': 3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open('/tmp/nbeats_tda_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nResults saved to: /tmp/nbeats_tda_results.json\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Training curves\n",
    "    axes[0, 0].plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(val_losses, label='Val Loss', alpha=0.8)\n",
    "    axes[0, 0].set_title('Training Curves', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('MSE Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Predictions vs Actual - FIXED: Show first time step only\n",
    "    sample_idx = slice(0, 200)\n",
    "    # Take only the first forecast step from each prediction window\n",
    "    first_step_preds = predictions_denorm[sample_idx, 0]  # Shape: (200,)\n",
    "    first_step_targets = targets_denorm[sample_idx, 0]    # Shape: (200,)\n",
    "    \n",
    "    axes[0, 1].plot(first_step_preds, label='Predicted (1-step)', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    axes[0, 1].plot(first_step_targets, label='Actual', linewidth=2, alpha=0.7)\n",
    "    axes[0, 1].set_title('Predictions vs Actual (First Step of Each Window)', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Sample')\n",
    "    axes[0, 1].set_ylabel('Price ($)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution - FIXED: Flatten properly\n",
    "    errors = np.abs(targets_denorm - predictions_denorm)\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    axes[1, 0].hist(errors_flat, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(np.mean(errors_flat), color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Mean: ${np.mean(errors_flat):.2f}')\n",
    "    axes[1, 0].set_xlabel('Absolute Error ($)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Error Distribution (All Forecast Steps)', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter: Actual vs Predicted - FIXED: Flatten properly\n",
    "    axes[1, 1].scatter(targets_denorm.flatten(), predictions_denorm.flatten(), \n",
    "                      alpha=0.3, s=10, c=errors_flat, cmap='RdYlGn_r')\n",
    "    min_val = min(targets_denorm.min(), predictions_denorm.min())\n",
    "    max_val = max(targets_denorm.max(), predictions_denorm.max())\n",
    "    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "    axes[1, 1].set_xlabel('Actual Price ($)')\n",
    "    axes[1, 1].set_ylabel('Predicted Price ($)')\n",
    "    axes[1, 1].set_title('Actual vs Predicted (All Steps, Color=Error)', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "    cbar.set_label('Error ($)', rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/nbeats_tda_results.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Plots saved to: /tmp/nbeats_tda_results.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNBEATS+TDA training complete!\")\n",
    "    print(\"\\nTo compare with vanilla NBEATS:\")\n",
    "    print(\"   1. Note these metrics\")\n",
    "    print(\"   2. Run vanilla NBEATS separately\")\n",
    "    print(\"   3. Calculate improvement percentages\")\n",
    "    \n",
    "    return model, metrics, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics, results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
